{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\glawl\\AppData\\Local\\Temp\\ipykernel_15028\\3573853591.py:8: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  matplotlib.style.use(\"seaborn\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad8f0eac8364474874df9b3b991a772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "matplotlib.style.use(\"seaborn\")\n",
    "\n",
    "# Relu function for the hidden layers\n",
    "def relu(x, grad=False):\n",
    "    \"\"\"\n",
    "    Implements the ReLU (Rectified Linear Unit) activation function.\n",
    "\n",
    "    Args:\n",
    "        x: Input value(s).\n",
    "        grad: Boolean flag indicating whether to return the gradient (True) or activation (False).\n",
    "\n",
    "    Returns:\n",
    "        ReLU activation of x if grad is False, otherwise the ReLU gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply ReLU function: max(0, x)\n",
    "    return np.maximum(0, x) if not grad else np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "# Softmax function for output layer\n",
    "def softmax(x, grad=False):\n",
    "    \"\"\"\n",
    "    Implements the softmax function for classification problems.\n",
    "\n",
    "    Args:\n",
    "        x: Input value(s).\n",
    "        grad: Boolean flag indicating whether to return the gradient (True) or activation (False).\n",
    "\n",
    "    Returns:\n",
    "        Softmax activation of x if grad is False, otherwise the softmax gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the exponentials of the input values for numerical stability\n",
    "    exp = np.exp(np.maximum(np.minimum(x, 8), -8))\n",
    "\n",
    "    # Prevent division by zero by adding a small constant to the denominator\n",
    "    denominator = np.sum(exp, axis=0) + 1e-3\n",
    "\n",
    "    # Calculate the softmax probabilities\n",
    "    s = exp / denominator\n",
    "\n",
    "    # Return activation or gradient based on the grad flag\n",
    "    return s if not grad else np.multiply(s, 1. - s)\n",
    "\n",
    "# Cross-entropy loss function\n",
    "def cross_entropy(x, y, grad=False):\n",
    "    \"\"\"\n",
    "    Calculates the cross-entropy loss between predicted (x) and true labels (y).\n",
    "\n",
    "    Args:\n",
    "        x: Predicted probabilities.\n",
    "        y: True labels (one-hot encoded).\n",
    "        grad: Boolean flag indicating whether to return the gradient (True) or loss (False).\n",
    "\n",
    "    Returns:\n",
    "        Cross-entropy loss if grad is False, otherwise the gradient of the loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clip target labels to avoid issues with log(0)\n",
    "    x = np.clip(x, 1e-10, 1. - 1e-10)\n",
    "\n",
    "    # Calculate cross-entropy loss\n",
    "    if grad:\n",
    "        return x - y  # Gradient of cross-entropy loss\n",
    "\n",
    "    return -np.sum(y * np.log(x), axis=0)\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Represents a basic neural network layer.\n",
    "\n",
    "    Attributes:\n",
    "        input_dim: Dimensionality of the input data.\n",
    "        output_dim: Dimensionality of the output data.\n",
    "        activation: Activation function to be applied to the layer's output.\n",
    "        w: Weight matrix of the layer, initialized with random values.\n",
    "        grad_w: Gradient of the weight matrix, used for training.\n",
    "        x: Input data to the layer (stored for backpropagation).\n",
    "        z: Weighted sum of the input before activation (stored for backpropagation).\n",
    "        a: Activated output of the layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, activation=relu):\n",
    "        \"\"\"\n",
    "        Initializes a new Layer object.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimensionality of the input data.\n",
    "            output_dim: Dimensionality of the output data.\n",
    "            activation: Activation function to be applied to the layer's output (default: relu)\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weight matrix with Xavier initialization for better convergence\n",
    "        self.w = np.random.normal(scale=1.0 / np.sqrt(input_dim), size=(output_dim, input_dim)).astype(np.float32)\n",
    "        self.grad_w = np.zeros_like(self.w).astype(np.float32)\n",
    "        self.x = None\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the layer.\n",
    "\n",
    "        Args:\n",
    "            x: Input data to the layer.\n",
    "\n",
    "        Returns:\n",
    "            Activated output of the layer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.x = x  # Store the input for backpropagation\n",
    "        self.z = np.dot(self.w, x)  # Calculate the weighted sum\n",
    "        self.a = self.activation(self.z)  # Apply activation function\n",
    "        return self.a\n",
    "    \n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Represents a basic neural network architecture.\n",
    "\n",
    "    Attributes:\n",
    "        learning_rate: Learning rate for gradient updates during training.\n",
    "        batch_size: Size of the data batch used for training.\n",
    "        layers: List of `Layer` objects representing the network's layers.\n",
    "        predictions: Network's predicted outputs during the last forward pass (internal use).\n",
    "        actuals: True labels during the last forward pass (internal use).\n",
    "        current_loss: Loss value calculated during the last forward pass (internal use).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, batch_size=32, loss_function=cross_entropy):\n",
    "        \"\"\"\n",
    "        Initializes a new NeuralNetwork object.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: Learning rate for gradient updates during training (default: 0.01).\n",
    "            batch_size: Size of the data batch used for training (default: 32).\n",
    "            loss_function: The function used to calculate loss (default: cross_entropy).\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_function = loss_function\n",
    "        self.layers = []  # List to hold network layers\n",
    "\n",
    "        # Internal variables to store network outputs for loss calculation\n",
    "        self.predictions = None\n",
    "        self.actuals = None\n",
    "        self.current_loss = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x: Input data to the network.\n",
    "\n",
    "        Returns:\n",
    "            Activated output of the last layer in the network.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # Pass input through each layer\n",
    "        return x\n",
    "\n",
    "    def __add__(self, layer):\n",
    "        \"\"\"\n",
    "        Efficiently adds a Layer object to the network.\n",
    "\n",
    "        Args:\n",
    "            layer: The Layer object to be added to the network.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If the input and output dimensions of consecutive layers are incompatible.\n",
    "        \"\"\"\n",
    "        if isinstance(layer, Layer):\n",
    "            if not self.layers:\n",
    "                self.layers.append(layer)\n",
    "            else:\n",
    "                # Ensure compatible dimensions between layers\n",
    "                assert layer.w.shape[1] == self.layers[-1].w.shape[0], \"Incompatible layer dimensions!\"\n",
    "                self.layers.append(layer)\n",
    "        return self\n",
    "\n",
    "    def loss(self, predictions, actuals):\n",
    "        \"\"\"\n",
    "        Calculates and stores the loss between predicted and actual outputs.\n",
    "\n",
    "        Args:\n",
    "            predictions: Network's predicted outputs.\n",
    "            actuals: True labels for the data.\n",
    "\n",
    "        Returns:\n",
    "            The calculated loss value.\n",
    "        \"\"\"\n",
    "        self.predictions = predictions\n",
    "        self.actuals = actuals\n",
    "        self.current_loss = np.mean(self.loss_function(predictions, actuals))  # Average cross-entropy loss\n",
    "        return self.current_loss\n",
    "\n",
    "    def backwards(self):\n",
    "        \"\"\"\n",
    "        Performs backpropagation to update weights of the network based on the calculated loss.\n",
    "        \"\"\"\n",
    "        # Calculate the gradient of the loss with respect to the network's predictions\n",
    "        loss_grad = cross_entropy(self.predictions, self.actuals, grad=True)\n",
    "        # Calculate the gradient of the activation function of the last layer\n",
    "        activation_grad = self.layers[-1].activation(self.layers[-1].z, grad=True)\n",
    "\n",
    "        # Compute the delta, which is the product of the loss gradient and activation gradient\n",
    "        delta = loss_grad * activation_grad\n",
    "        # Reshape delta for compatibility with matrix multiplication\n",
    "        delta = delta.T.reshape(self.batch_size, -1, 1)\n",
    "\n",
    "        # Compute the gradient of the weights of the last layer\n",
    "        prev_activation = self.layers[-1].x.T.reshape(self.batch_size, 1, -1)\n",
    "        self.layers[-1].dw = np.mean(delta * prev_activation, axis=0)\n",
    "\n",
    "        # Backpropagate through the layers, starting from the second-to-last layer\n",
    "        for i in range(2, len(self.layers) + 1):\n",
    "            # Transpose weights for matrix multiplication\n",
    "            weights_transpose = self.layers[-i + 1].w.transpose()\n",
    "\n",
    "            # Compute the gradient of the activation function of the current layer\n",
    "            z = self.layers[-i].z\n",
    "            activation_grad = self.layers[-i].activation(z, grad=True)\n",
    "            activation_grad = activation_grad.T.reshape(self.batch_size, -1, 1)\n",
    "\n",
    "            # Update delta using the chain rule\n",
    "            delta = np.matmul(weights_transpose, delta) * activation_grad\n",
    "\n",
    "            # Compute the gradient of the weights of the current layer\n",
    "            prev_activation = self.layers[-i].x.T.reshape(self.batch_size, 1, -1)\n",
    "            self.layers[-i].dw = np.mean(np.matmul(delta, prev_activation), axis=0)\n",
    "\n",
    "        # Update weights of all layers using gradient descent\n",
    "        for layer in self.layers:\n",
    "            layer.w = layer.w - self.learning_rate * layer.dw\n",
    "\n",
    "\n",
    "file = r\"MNIST_CSV\\mnist_train.csv\"\n",
    "data = pd.read_csv(file, header=None).values.astype(np.float32)\n",
    "samples = [(data[i, 1:] / 255, np.eye(10)[int(data[i, 0])].astype(np.float32)) for i in range(len(data))]\n",
    "\n",
    "np.random.seed(10_000)\n",
    "random.seed(10_000)\n",
    "\n",
    "model = NeuralNetwork(learning_rate=.95, batch_size=64)\n",
    "l1 = Layer(784, 50, relu)\n",
    "l2 = Layer(50, 20, relu)\n",
    "l3 = Layer(20, 10, softmax)\n",
    "\n",
    "model += l1\n",
    "model += l2\n",
    "model += l3\n",
    "\n",
    "epochs = 10_000\n",
    "losses = []\n",
    "for i in trange(epochs, ncols=1000):\n",
    "    batch = random.sample(samples, model.batch_size)\n",
    "    X = np.column_stack([b[0] for b in batch]).astype(np.float32)\n",
    "    Y = np.column_stack([b[1] for b in batch]).astype(np.float32)\n",
    "\n",
    "    pred = model(X)\n",
    "    loss = model.loss(pred, Y)\n",
    "    losses.append(loss)\n",
    "    model.backwards()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Batch Error\")\n",
    "plt.title(\"Training Error through Time\")\n",
    "plt.savefig('error.png', dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
